{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Enwiki Train\n",
    "\n",
    "Test that the model init code, runs without issues\n",
    "\n",
    "**L6-D512 model with**\n",
    "- Layer count: 6\n",
    "- Embed size: 512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "## Preparing the init model and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: False\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-unit-test\n",
      "TRAINER_DIR: /home/ubuntu/rwkv-proj/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /home/ubuntu/rwkv-proj/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"infctx-v5-unit-test\"\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the various directories\n",
    "!mkdir -p \"{PROJECT_DIR}/model/\"\n",
    "!mkdir -p \"{PROJECT_DIR}/datapath/\"\n",
    "!mkdir -p \"{PROJECT_DIR}/checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-15 04:13:08,468] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-world-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Lets initialized the L6-D512 model with the init_model.py code\n",
    "!cd \"{TRAINER_DIR}\" && python3 init_model.py \\\n",
    "    --n_layer 6 --n_embd 512 \\\n",
    "    --vocab_size world \\\n",
    "    --skip-if-exists --safe-init \\\n",
    "    ../model/L6-D512-world-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Saving the dataset (1/1 shards): 100%|█| 751/751 [00:00<00:00, 10414.72 examples\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 2308.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Preload the dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/enwiki_10k-world-4x1024.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-16 07:05:12,909] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-unit-test/config/enwiki_10k-world-4x1024.yaml', '--trainer.logger.init_args.name=infctx-v5-unit-test (train-ctx=1024, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.fast_dev_run=2', '--model.load_model=../model/L6-D512-world-init.pth'], args=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-unit-test/config/enwiki_10k-world-4x1024.yaml', '--trainer.logger.init_args.name=infctx-v5-unit-test (train-ctx=1024, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--trainer.fast_dev_run=2', '--model.load_model=../model/L6-D512-world-init.pth'].\n",
      "Seed set to 3941088705\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 2 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         2\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Saving the dataset (1/1 shards): 100%|█| 751/751 [00:00<00:00, 12700.28 examples\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 2531.07 examples/s]\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory ../checkpoint/infctx-v5-unit-test-baseline-4x1024 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06854009628295898 seconds\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██████████████████| 2/2 [00:04<00:00,  0.42it/s, train/loss=11.20]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 1/2 [00:02<00:02,  0.44it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 2/2 [00:04<00:00,  0.44it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 2/2 [00:09<00:00,  0.21it/s, train/loss=11.20, validation/loss=`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 2/2 [00:09<00:00,  0.21it/s, train/loss=11.20, validation/loss=\n"
     ]
    }
   ],
   "source": [
    "# Short training process - for quick testing / debugging\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"disabled\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-4x1024.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (train-ctx=1024, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.fast_dev_run=2 \\\n",
    "        --model.load_model=\"../model/L6-D512-world-init.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty out the checkpoint\n",
    "!cd \"{PROJECT_DIR}\" && rm -rf \"./checkpoint/infctx-v5-unit-test-baseline-4x1024/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-16 07:46:13,116] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-unit-test/config/enwiki_10k-world-4x1024.yaml', '--trainer.logger.init_args.name=infctx-v5-unit-test (train-ctx=1024, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.load_model=../model/L6-D512-world-init.pth'], args=['fit', '-c', '/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/notebook/trainer-v5-unit-test/config/enwiki_10k-world-4x1024.yaml', '--trainer.logger.init_args.name=infctx-v5-unit-test (train-ctx=1024, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.load_model=../model/L6-D512-world-init.pth'].\n",
      "Seed set to 3941088705\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         2\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Saving the dataset (1/1 shards): 100%|█| 751/751 [00:00<00:00, 12071.49 examples\n",
      "Saving the dataset (1/1 shards): 100%|███| 8/8 [00:00<00:00, 2501.45 examples/s]\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:630: Checkpoint directory ../checkpoint/infctx-v5-unit-test-baseline-4x1024 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0669260025024414 seconds\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0:  21%|▋  | 80/376 [00:25<01:33,  3.15it/s, v_num=y1d5, train/loss=8.000]/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██| 376/376 [02:19<00:00,  2.71it/s, v_num=y1d5, train/loss=6.690]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▌                 | 1/8 [00:02<00:18,  0.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|█████               | 2/8 [00:04<00:14,  0.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███████▌            | 3/8 [00:07<00:11,  0.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 4/8 [00:09<00:09,  0.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|████████████▌       | 5/8 [00:11<00:07,  0.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████████     | 6/8 [00:13<00:04,  0.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|█████████████████▌  | 7/8 [00:16<00:02,  0.43it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 8/8 [00:18<00:00,  0.43it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 376/376 [02:37<00:00,  2.39it/s, v_num=y1d5, train/loss=6.690, \u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 376/376 [02:43<00:00,  2.31it/s, v_num=y1d5, train/loss=6.690, \n"
     ]
    }
   ],
   "source": [
    "# Longer training process\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_10k-world-4x1024.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (train-ctx=1024, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.microbatch_size=8 \\\n",
    "        --model.load_model=\"../model/L6-D512-world-init.pth\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-15 07:01:27,247] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/infctx-v5-unit-test-baseline-4x1024/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.12.0\n",
      "Reconstructed fp32 state dict with 138 params 87601152 elements\n",
      "Saving bf16 state dict to ../model/infctx-v5-unit-test-baseline-4x1024.pth\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 168M Nov 15 07:01 ../model/infctx-v5-unit-test-baseline-4x1024.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/infctx-v5-unit-test-baseline-4x1024/last.ckpt\" \"../model/infctx-v5-unit-test-baseline-4x1024.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/infctx-v5-unit-test-baseline-4x1024.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-15 07:01:43,514] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0'\n",
      "[SimpleRWKV] Warning: dtype mismatch, only fp32 is supported (for now)\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "====================================================================\n",
      "[WARNING]: bptt_truncated_learning is set as true (was configured as false), due to incomplete implementation of CUDA kernel for bptt_learning\n",
      "====================================================================\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "/home/ubuntu/rwkv-proj/RWKV-infctx-trainer/RWKV-v5/src/model.py:1272: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_tokens = torch.tensor(\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. When he TelOn her government two hours of Maualdkies's movementi took with type of a arrival of women and Public airport were from heavy water because Christ at become Billboarduse cross. The itsrian wild, stating and other monsternia atnyward13 a single series. In April 19 about New York Pees in war as attacked from the instrument for 1, you Ham released its massacre, in army a School of he parts, so we 1917 to was's diary's most built on a Series (June was around used the 3u, theyki2 to doing the regular General classm\" of her holyetteira found the current only domesticnelal estate and acknowledged by him by the Monte waterg Corporation ( Gambk University), causing the content her writer show until Marine February, in Christ's BBC face November 19025 approachk Mem spent other living ends most competition company was mostness party in it into existence to removed in their ‘Release From to May his political\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 dragon_test.py \"../model/infctx-v5-unit-test-baseline-4x1024.pth\" \"cuda fp32\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
